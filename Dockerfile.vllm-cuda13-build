# Dockerfile.vllm-cuda13-build - vLLM container with PyTorch built from source for CUDA 13.1/sm_121
#
# Purpose: Full sm_121 (GB10/DGX Spark) support with vLLM built from source
# Base: CUDA 13.1.0 devel
# PyTorch: Built from source with TORCH_CUDA_ARCH_LIST=12.1
# vLLM: Built from source (main branch)
#
# Build (takes 2-3 hours):
#   docker build -f Dockerfile.vllm-cuda13-build -t vllm:cuda13-full .
#
# Run:
#   docker run --rm --gpus all --ipc=host \
#     -v $(pwd)/models:/workspace/models \
#     vllm:cuda13-full \
#     python -m vllm.entrypoints.openai.api_server \
#       --model /workspace/models/Nemotron-3-Nano-30B-A3B-BF16 \
#       --port 8000

FROM nvidia/cuda:13.1.0-devel-ubuntu24.04

LABEL maintainer="nemotron-speech"
LABEL description="vLLM container with PyTorch built from source for CUDA 13.1 sm_121"
LABEL version="1.0"

# Prevent interactive prompts
ENV DEBIAN_FRONTEND=noninteractive

# =============================================================================
# System Dependencies
# =============================================================================
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.12 \
    python3.12-dev \
    python3.12-venv \
    python3-pip \
    git \
    curl \
    wget \
    cmake \
    ninja-build \
    ccache \
    libopenblas-dev \
    libomp-dev \
    libffi-dev \
    libssl-dev \
    libnuma-dev \
    && rm -rf /var/lib/apt/lists/* \
    && ln -sf /usr/bin/python3.12 /usr/bin/python3 \
    && ln -sf /usr/bin/python3 /usr/bin/python

# Install uv for fast package management
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/
ENV UV_SYSTEM_PYTHON=1
ENV UV_BREAK_SYSTEM_PACKAGES=1

# =============================================================================
# cuDNN Installation (required for PyTorch CUDA)
# =============================================================================
COPY --from=nvcr.io/nvidia/pytorch:25.11-py3 /usr/lib/aarch64-linux-gnu/libcudnn* /usr/lib/aarch64-linux-gnu/
COPY --from=nvcr.io/nvidia/pytorch:25.11-py3 /usr/include/cudnn* /usr/include/

# =============================================================================
# NCCL Libraries (required for PyTorch distributed)
# =============================================================================
COPY --from=nvcr.io/nvidia/pytorch:25.11-py3 /usr/lib/aarch64-linux-gnu/libnccl* /usr/lib/aarch64-linux-gnu/
COPY --from=nvcr.io/nvidia/pytorch:25.11-py3 /usr/include/nccl.h /usr/include/
COPY --from=nvcr.io/nvidia/pytorch:25.11-py3 /usr/include/nccl_device.h /usr/include/
COPY --from=nvcr.io/nvidia/pytorch:25.11-py3 /usr/include/nccl_device/ /usr/include/nccl_device/

# Verify NCCL is properly installed
RUN echo "=== Verifying NCCL installation ===" && \
    ls -la /usr/lib/aarch64-linux-gnu/libnccl* && \
    ls -la /usr/include/nccl.h && \
    ldconfig

# =============================================================================
# PyTorch Build Dependencies
# =============================================================================
RUN uv pip install --no-cache \
    numpy \
    pyyaml \
    typing_extensions \
    sympy \
    filelock \
    networkx \
    jinja2 \
    fsspec \
    packaging \
    setuptools \
    wheel \
    cffi \
    future \
    requests \
    dataclasses \
    pillow \
    expecttest \
    hypothesis \
    pytest

# =============================================================================
# Build PyTorch from Source
# =============================================================================
# Cache buster - increment to force PyTorch rebuild
ARG PYTORCH_CACHE_BUSTER=v1-vllm

WORKDIR /build

# Clone PyTorch main branch (needed for CUDA 13.1 compatibility)
RUN git clone --recursive --depth 1 https://github.com/pytorch/pytorch.git

WORKDIR /build/pytorch

# Update submodules
RUN git submodule sync && git submodule update --init --recursive --depth 1

# Set build environment for sm_121 (GB10)
ENV TORCH_CUDA_ARCH_LIST="12.1"
ENV USE_CUDA=1
ENV USE_CUDNN=1
ENV USE_MKLDNN=1
ENV USE_DISTRIBUTED=1
ENV USE_NCCL=1
ENV USE_TENSORPIPE=0
ENV USE_SYSTEM_NCCL=1
ENV NCCL_ROOT=/usr
ENV NCCL_LIB_DIR=/usr/lib/aarch64-linux-gnu
ENV NCCL_INCLUDE_DIR=/usr/include
ENV BUILD_TEST=0
ENV MAX_JOBS=8
ENV CMAKE_BUILD_TYPE=Release
ENV CUDNN_LIB_DIR=/usr/lib/aarch64-linux-gnu
ENV CUDNN_INCLUDE_DIR=/usr/include
ENV USE_PRIORITIZED_TEXT_FOR_LD=1

# Create symlinks for CUB/Thrust from CCCL (CUDA 13.1 layout)
RUN ln -sf /usr/local/cuda/include/cccl/cub /usr/local/cuda/include/cub && \
    ln -sf /usr/local/cuda/include/cccl/thrust /usr/local/cuda/include/thrust && \
    echo "=== Verifying CUB access ===" && \
    ls /usr/local/cuda/include/cub/cub.cuh && \
    ls /usr/local/cuda/include/cccl/cub/cub.cuh

# Build and install PyTorch
ENV CUB_INCLUDE_DIR=/usr/local/cuda/include/cccl
RUN python3 setup.py bdist_wheel && \
    uv pip install --no-cache dist/*.whl && \
    mkdir -p /tmp/pytorch_wheel && \
    cp dist/*.whl /tmp/pytorch_wheel/

# =============================================================================
# Cleanup PyTorch build to save space
# =============================================================================
WORKDIR /workspace
RUN rm -rf /build/pytorch

# =============================================================================
# vLLM Dependencies
# =============================================================================
RUN uv pip install --no-cache \
    transformers>=4.45.0 \
    tokenizers>=0.19 \
    sentencepiece \
    fastapi \
    uvicorn[standard] \
    pydantic>=2.0 \
    prometheus_client \
    py-cpuinfo \
    tiktoken \
    lm-format-enforcer \
    outlines \
    xgrammar \
    typing_extensions>=4.10 \
    filelock \
    pyzmq \
    msgspec \
    gguf \
    compressed-tensors \
    importlib_metadata \
    mistral_common>=1.5.0 \
    partial-json-parser

# =============================================================================
# Build vLLM from Source
# =============================================================================
ARG VLLM_CACHE_BUSTER=v1

WORKDIR /build

# Clone vLLM main branch
RUN git clone --depth 1 https://github.com/vllm-project/vllm.git

WORKDIR /build/vllm

# Set vLLM build environment for sm_121
ENV TORCH_CUDA_ARCH_LIST="12.1"
ENV VLLM_TARGET_DEVICE="cuda"
ENV MAX_JOBS=8

# Install vLLM build dependencies
RUN uv pip install --no-cache \
    cmake>=3.26 \
    ninja \
    packaging \
    setuptools>=61 \
    setuptools-scm>=8 \
    wheel \
    jinja2

# Build and install vLLM
RUN pip install --break-system-packages -e . --no-build-isolation

# Reinstall our custom PyTorch built for sm_121 (vLLM may have replaced it)
# Also remove torchvision/torchaudio which are incompatible with our custom PyTorch
RUN pip uninstall -y torchvision torchaudio || true && \
    pip install --break-system-packages --force-reinstall /tmp/pytorch_wheel/torch*.whl

# Install triton and fix ptxas for sm_121a support
RUN pip install --break-system-packages triton && \
    rm -f /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas && \
    ln -s /usr/local/cuda/bin/ptxas /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas

# =============================================================================
# Cleanup build artifacts
# =============================================================================
WORKDIR /workspace
# Keep vLLM source for editable install
# RUN rm -rf /build/vllm

# =============================================================================
# Environment
# =============================================================================
ENV PYTHONUNBUFFERED=1
ENV VLLM_LOGGING_LEVEL=INFO
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH}
ENV HF_HOME=/workspace/.cache/huggingface
# Disable libuv (not built into our custom PyTorch)
ENV USE_LIBUV=0

# Create model directory
RUN mkdir -p /workspace/models

EXPOSE 8000

# Default command
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--help"]
